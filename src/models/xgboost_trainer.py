import os
import sys
import joblib
from scipy.sparse import hstack
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier

# Ensure dataset_loader.py can be imported correctly
BASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
sys.path.append(os.path.join(BASE_DIR, "src"))

from data.dataset_loader import load_dataset

# ğŸš€ Load dataset (Both text & numerical features)
X_train_text, X_test_text, X_train_num, X_test_num, y_train, y_test = load_dataset()

# ğŸš€ Debug: Check dataset structure
print("\nğŸ” Dataset Type Check:")
print(f"   - X_train_text type: {type(X_train_text)}, shape: {X_train_text.shape}")
print(f"   - X_train_num type: {type(X_train_num)}, shape: {X_train_num.shape}")
print(f"   - y_train type: {type(y_train)}, shape: {y_train.shape}")

# ğŸš€ Convert text to numerical features using TF-IDF
vectorizer = TfidfVectorizer(max_features=10000)  # ğŸ”¹ Increase TF-IDF features for better accuracy
X_train_text_tfidf = vectorizer.fit_transform(X_train_text)
X_test_text_tfidf = vectorizer.transform(X_test_text)

# ğŸš€ Combine TF-IDF features with numerical features
X_train_combined = hstack([X_train_text_tfidf, X_train_num])  # Combine sparse TF-IDF and dense numerical features
X_test_combined = hstack([X_test_text_tfidf, X_test_num])

# ğŸš€ Debug: Print dataset shape before applying SMOTE
print("\nâœ… Dataset before SMOTE balancing:")
print(f"   - X_train_combined shape: {X_train_combined.shape}")
print(f"   - y_train shape: {y_train.shape}")

# Apply SMOTE to balance the dataset
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_combined, y_train)

# ğŸš€ Debug: Print dataset shape after SMOTE
print("\nâœ… Dataset balanced using SMOTE.")
print(f"   - Original dataset: {X_train_combined.shape[0]} samples")
print(f"   - Resampled dataset: {X_train_resampled.shape[0]} samples")

# ğŸš€ Define Hyperparameter Grid for GridSearch
param_grid = {
    'n_estimators': [100, 200, 300],  
    'max_depth': [3, 6, 9],  
    'learning_rate': [0.01, 0.1, 0.2],  
    'subsample': [0.8, 1.0],  
    'colsample_bytree': [0.8, 1.0]  
}

# ğŸš€ Perform Grid Search to find the best hyperparameters
print("\nğŸ” Running Grid Search for best hyperparameters...")
grid_search = GridSearchCV(
    XGBClassifier(objective="binary:logistic"),
    param_grid,
    cv=5, 
    scoring='accuracy', 
    n_jobs=-1
)
grid_search.fit(X_train_resampled, y_train_resampled)

# ğŸš€ Use best model from Grid Search
model = grid_search.best_estimator_
print(f"\nâœ… Best Hyperparameters: {grid_search.best_params_}")

# ğŸš€ Train XGBoost Classifier with Best Hyperparameters
print("\nğŸš€ Training XGBoost Classifier...")
model.fit(X_train_resampled, y_train_resampled)

# ğŸš€ Evaluate Model
y_pred = model.predict(X_test_combined)
accuracy = accuracy_score(y_test, y_pred)

print(f"\nâœ… Model Training Completed. Accuracy: {accuracy * 100:.2f}%")
print("\nğŸ” Classification Report:\n", classification_report(y_test, y_pred))

# ğŸš€ Ensure the model directory exists before saving
model_dir = os.path.join(BASE_DIR, "src/models/model")
os.makedirs(model_dir, exist_ok=True)

# ğŸš€ Save the Model and Vectorizer
model_save_path = os.path.join(model_dir, "xgboost_model.pkl")
vectorizer_save_path = os.path.join(model_dir, "tfidf_vectorizer.pkl")

joblib.dump(model, model_save_path)
joblib.dump(vectorizer, vectorizer_save_path)

print(f"\nâœ… Model saved to `{model_save_path}`")
print(f"âœ… Vectorizer saved to `{vectorizer_save_path}`")
